{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9\n",
    "## Thursday, October 4th, 2018\n",
    "### Automatic Differentiation:  Just Beyond the Basics\n",
    "\n",
    "#### References\n",
    "* [A Hitchhikerâ€™s Guide to Automatic Differentiation](https://link.springer.com/article/10.1007/s11075-015-0067-6)\n",
    "* Griewank, A. and Walther, A., 2008. Evaluating derivatives: principles and techniques of algorithmic differentiation (Vol. 105). Siam.\n",
    "* Nocedal, J. and Wright, S., 2001. Numerical Optimization, Second Edition. Springer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries that we'll need\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "# or inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "Last time:\n",
    "* Provided introduction and motivation for automatic differentiation (AD)\n",
    "  - Root-finding (Newton's method)\n",
    "  - The Jacobian\n",
    "  - The finite difference method\n",
    "* The basics of AD\n",
    "  - The computational graph\n",
    "  - Evaluation trace\n",
    "  - Computing derivatives of a function of one variable using the forward mode\n",
    "  - Computing derivatives of a function of two variables using the forward mode\n",
    "\n",
    "# Today\n",
    "* The Jacobian in forward mode\n",
    "* What the forward mode actually computes\n",
    "* Dual numbers\n",
    "* Towards implementation\n",
    "* Thoughts on the reverse mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finishing up the Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on Notation\n",
    "A few students expressed concern with the \"dot\" notation from last time.  Admittedly, it is somewhat opaque notation.  However, it is also somewhat standard notation in the AD community (as much as any terminology is standard in the AD community).  You should feel free to use whatever notation makes you happy in this class.  Note, too, that when we start into the reverse mode, the itermediate steps are no longer denoted by a \"dot\".  In the reverse mode, intermediate steps are denoted with an \"overbar\".  This differing notation helps in distinguishing the two modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards the Jacobian\n",
    "Time to take a step back and recap.  In the very beginning of the lecture, we motivated calculation of derivatives by Newton's method for solving a system of nonlinear equations.  One of the key elements in that method is the Jacobian matrix $J = \\dfrac{\\partial f_{i}}{\\partial x_{j}}$.  So far, you have learned how to manually perform the forward mode of automatic differentiation both for scalar functions of a single variable and scalar functions of multiple variables.  The solution of systems of equations requires differentation of a vector-function of multiple variables.\n",
    "\n",
    "### Some Notation and the Seed Vector\n",
    "Note that every time we computed the derivative using the forward mode, we \"seeded\" the derivative with a value.  We chose this value to be unity.  Let's look at a very simple function $f\\left(x,y\\right) = xy$.  Clearly $$\\nabla f = \\begin{bmatrix} y \\\\ x \\end{bmatrix}.$$  The evaluation trace consists of $f = x_{3} = x_{1}x_{2}$ where $x_{1}$ and $x_{2}$ take on the values at the point we wish to evaluate the function and its derivative.  Let's introduce a seed vector $p$ and define the directional derivative to be $$D_{p}x_{3} = \\sum_{j=1}^{2}{\\dfrac{\\partial x_{3}}{\\partial x_{j}}p_{j}}.$$  Expanding the sum gives\n",
    "\\begin{align}\n",
    "  D_{p}x_{3} &= \\dfrac{\\partial x_{3}}{\\partial x_{1}}p_{1} + \\dfrac{\\partial x_{3}}{\\partial x_{2}}p_{2} \\\\\n",
    "             &= x_{2}p_{1} + x_{1}p_{2}.\n",
    "\\end{align}\n",
    "Notice that if we choose the seed vector to be $p = \\left(1,0\\right)$ we get $\\dfrac{\\partial f}{\\partial x}$ and if we choose $p = \\left(0,1\\right)$ we get $\\dfrac{\\partial f}{\\partial y}$.  Of course, we aren't required to choose the seed vectors to be unit vectors.  However, the utility should be apparent.\n",
    "\n",
    "Note that this could be written as $$D_{p}x_{3} = \\nabla x_{3}\\cdot p.$$  So, the forward mode of AD is really computing the *product* of the gradient of our function with the seed vector!\n",
    "\n",
    "### What Forward AD Acutally Computes\n",
    "The forward mode of automatic differentiation actually computes the product $\\nabla f \\cdot p$.  If $f$ is a vector, then the forward mode actually computes $Jp$ where $J = \\dfrac{\\partial f_{i}}{\\partial x_{j}}, \\quad i = 1,\\ldots, n, \\quad j = 1,\\ldots,m$ is the Jacobian matrix.  In many applications, we really only want the \"action\" of the Jacobian on a vector.  That is, we just want to compute the matrix-vector product $Jp$ for some vector $p$.  Of course, if we really want to form the entire Jacobian, this is readily performed by forming a set of $m$ unit seed vectors $\\left\\{p_{1},\\ldots p_{m}\\right\\}$ where $p_{k}$ consists of all zeros except for a single $1$ in position $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "That was a lot of jargon.  Let's do a little example to give the basic ideas.  Let\n",
    "\\begin{align}\n",
    "  f\\left(x,y\\right) = \n",
    "  \\begin{bmatrix}\n",
    "    xy + \\sin\\left(x\\right) \\\\\n",
    "    x + y + \\sin\\left(xy\\right)\n",
    "  \\end{bmatrix}.\n",
    "\\end{align}\n",
    "The computational graph will have two inputs $x$ and $y$ and two outputs this time $x_{7}$ and $x_{8}$.  In the notation below $x_{7}$ corresponds to $f_{1}$ and $x_{8}$ corresponds to $f_{2}$.  Let's start by computing the gradient of $f_{1}$.  Using the directional derivative we have \n",
    "\\begin{align}\n",
    "  D_{p}x_{7} = \\left(\\cos\\left(x_{1}\\right) + x_{2}\\right)p_{1} + x_{1}p_{2}.\n",
    "\\end{align}\n",
    "**Note:** You should fill in the details!\n",
    "Choosing $p = (1,0)$ gives $\\dfrac{\\partial f_{1}}{\\partial x}$ and choosing $p = (0,1)$ gives $\\dfrac{\\partial f_{1}}{\\partial y}$.  These form the first row of the Jacobian!  The second row of the Jacobian can be computed similarly by working with $x_{8}$.\n",
    "\n",
    "The take-home message is that we can form the full Jacobian by using $m$ seed vectors where $m$ is the number of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation and Dual Numbers\n",
    "A dual number is an extension of the real numbers.  Written out, the form looks similar to a complex number.\n",
    "\n",
    "## Review of Comlex Numbers\n",
    "Recall that a complex number has the form $$z = a + ib$$ where we *define* the number $i$ so that $i^{2} = -1$.  No real number has this property but it is a useful property for a number to have.  Hence the introduction of complex numbers.  Visually, you can think of a real number as a number lying on a straight line.  Then, we \"extend\" the real line \"up\".  The new axis is called the *imaginary* axis.\n",
    "\n",
    "Complex numbers have several properties that we can use.\n",
    "* Complex conjugate: $z^{*} = a - ib$.\n",
    "* Magnitude of a complex number: $\\left|z\\right|^{2} = zz^{*} = \\left(a+ib\\right)\\left(a-ib\\right) = a^{2} + b^{2}$.\n",
    "* Polar form: $z = \\left|z\\right|\\exp\\left(i\\theta\\right)$ where $\\displaystyle \\theta = \\tan^{-1}\\left(\\dfrac{b}{a}\\right)$.\n",
    "\n",
    "## Towards Dual Numbers\n",
    "A dual number has a real part and a dual part.  We write $$z = x + \\epsilon x^{\\prime}$$ and refer to $x^{\\prime}$ as the dual part.  We *define* the number $\\epsilon$ so that $\\epsilon^{2} = 0$.  **This does not mean that $\\epsilon$ is zero!**  $\\epsilon$ is not a real number.\n",
    "\n",
    "#### Some properties of dual numbers:\n",
    "* Conjugate:  $z^{*} = x - \\epsilon x^{\\prime}$.\n",
    "* Magnitude: $\\left|z\\right|^{2} = zz^{*} = \\left(x+\\epsilon x^{\\prime}\\right)\\left(x-\\epsilon x^{\\prime}\\right) = x^{2}$.\n",
    "* Polar form: $z = x\\left(1 + \\dfrac{x^{\\prime}}{x}\\right)$.\n",
    "\n",
    "### Example\n",
    "Recall that the derivative of $y=x^{2}$ is $y^{\\prime} = 2xx^{\\prime} = 2x$.\n",
    "\n",
    "Now if we extend $x$ so that it has a real part and a dual part ($x\\leftarrow x + \\epsilon x^{\\prime}$) and evaluate $y$ we have\n",
    "\\begin{align}\n",
    "  y &= \\left(x + \\epsilon x^{\\prime}\\right)^{2} \\\\\n",
    "    &= x^{2} + 2xx^{\\prime}\\epsilon + \\underbrace{x^{\\prime^{2}}\\epsilon^{2}}_{=0} \\\\\n",
    "    &= x^{2} + 2xx^{\\prime}\\epsilon.\n",
    "\\end{align}\n",
    "#### Notice that the dual part contains the derivative of our function!!\n",
    "\n",
    "### Example\n",
    "Evaluate $y = \\sin\\left(x\\right)$ when $x\\leftarrow x + \\epsilon x^{\\prime}$.\n",
    "\n",
    "We have\n",
    "\\begin{align}\n",
    "  y & = \\sin\\left(x + \\epsilon x^{\\prime}\\right) \\\\\n",
    "    & = \\sin\\left(x\\right)\\cos\\left(\\epsilon x^{\\prime}\\right) + \\cos\\left(x\\right)\\sin\\left(\\epsilon x^{\\prime}\\right).\n",
    "\\end{align}\n",
    "Expanding $\\cos$ and $\\sin$ in their Taylor series gives \n",
    "\\begin{align}\n",
    "  \\sin\\left(\\epsilon x^{\\prime}\\right) &= \\sum_{n=0}^{\\infty}{\\left(-1\\right)^{n}\\dfrac{\\left(\\epsilon x^{\\prime}\\right)^{2n+1}}{\\left(2n+1\\right)!}} = \\epsilon x^{\\prime} + \\dfrac{\\left(\\epsilon x^{\\prime}\\right)^{3}}{3!} + \\cdots = \\epsilon x^{\\prime} \\\\\n",
    "  \\cos\\left(\\epsilon x^{\\prime}\\right) &= \\sum_{n=0}^{\\infty}{\\left(-1\\right)^{n}\\dfrac{\\left(\\epsilon x^{\\prime}\\right)^{2n}}{\\left(2n\\right)!}} = 1 + \\dfrac{\\left(\\epsilon x^{\\prime}\\right)^{2}}{2} + \\cdots = 1.\n",
    "\\end{align}\n",
    "Note that the definition of $\\epsilon$ was used which resulted in the collapsed sum.\n",
    "\n",
    "So we see that \n",
    "\\begin{align}\n",
    "  y & = \\sin\\left(x\\right) + \\cos\\left(x\\right) x^{\\prime} \\epsilon.\n",
    "\\end{align}\n",
    "And once again the real component is the function and the dual component is the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Using dual numbers, find the derivative of $$ y = e^{x^{2}}.$$  **Show your work!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts on Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways of implementing automatic differentiation.  Two predominant approaches are code translation through pre-processor directives and operator-overloading.  We won't much to say about code translation.  It can be quite complicated, even going so far as to write a compiler.  The high-level idea is that the compiler writes additional code to compute derivatives of functions.  The benefits are that it can be very efficient and fast.\n",
    "\n",
    "Our approach will use operator overloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator Overloading\n",
    "The Wikipedia article on [operator overloading](https://en.wikipedia.org/wiki/Operator_overloading) states: \n",
    "> ...operator overloading, sometimes termed operator ad hoc polymorphism, is a specific case of polymorphism, where different operators have different implementations depending on their arguments.\n",
    "\n",
    "You have already seen the basics of this although we didn't call it operator overloading.  `python` allows operator overloading via the special methods.  When you wrote the circle class, some of you tried to define the `__add__` method (and `__radd__` for communtivity) to add two circle objects.  In this way, you *overloaded* the additon operator (`+`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let's create a class to do complex numbers again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Complex' and 'Complex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5e457e1e9eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# And add them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Complex' and 'Complex'"
     ]
    }
   ],
   "source": [
    "# Complex class again\n",
    "class Complex():\n",
    "    def __init__(self, a, b):\n",
    "        # Constructor to set up real and imag parts\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "# Create some complex numbers\n",
    "z1 = Complex(0, 1)\n",
    "z2 = Complex(2, 0)\n",
    "\n",
    "# And add them\n",
    "z = z1 + z2\n",
    "print(z.a, z.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work.  `python` doesn't know how to apply the addition operator (`+`) to `Complex` objects!  Let's remedy that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "# Complex class again\n",
    "class Complex():\n",
    "    def __init__(self, a, b):\n",
    "        # Constructor to st up real and imag parts\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        # Implementating complex addition\n",
    "        return Complex(self.a + other.a, self.b + other.b)\n",
    "\n",
    "z1 = Complex(0, 1)\n",
    "z2 = Complex(2, 0)\n",
    "z = z1 + z2\n",
    "print(z.a, z.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, much better.  Of course, there are still many problems here:\n",
    "* No convenient way to print out a complex number (where's `__str__`?)\n",
    "* What happens when you try to add a complex number to a real number (e.g. `2 + z`)?\n",
    "  - Try it!\n",
    "  - Then check out `__radd__`.\n",
    "  - This is how you get commutivity.\n",
    "* Many operations are missing (e.g. where's multiplication?).\n",
    "\n",
    "Nevertheless, the addition operator has been (partly) successfully overloaded (we didn't get commutivity yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with the Duals\n",
    "This section will give you a hint on how you may go about implementing the forward mode.  I will be deliberately terse.  In fact, I will not even define a class or use any operator overloading.  What I'm about to show you is almost purely conceptual.  You will do the hard work of coming up with a user-friendly implementation for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a baby example.  Suppose at some point in our evaluate trace we have $$x_{3} = x_{1}x_{2}.$$  We want to evaluate $x_{3}$ and the derivative.  Following our usual conventions we have $$\\dot{x}_{3} = x_{1}\\dot{x}_{2} + \\dot{x}_{1}x_{2}.$$  For our purposes here, suppose we have that $x_{1} = 2$ and $x_{2} = 3$ as well as $\\dot{x}_{1} = 1$ and $\\dot{x}_{2} = 4$.  It doesn't matter where these values came from at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an object that has two components.  The first component will hold the value of the function and the second component will hold the value of the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     x1   x1 dot\n",
    "x1 = [2,     1] # value and derivative\n",
    "x2 = [3,     4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear how to compute the value of the function at the point $a$.  Simply use the first component of our object to compute the function value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 0]\n"
     ]
    }
   ],
   "source": [
    "f = [0, 0] # Allocate the values of the function and its derivative\n",
    "# f(a) = x1    * x2\n",
    "f[0]   = x1[0] * x2[0]\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to calculate the derivative, we need to *overload* multiplication.  Any time we encounter a multiplication of something in the second slot, we need to use the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 11]\n"
     ]
    }
   ],
   "source": [
    "#f'(a) = x1    * x2dot + x1dot * x2\n",
    "f[1]   = x1[0] * x2[1] + x1[1] * x2[0]\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap.  We'll use OOP vernacular.\n",
    "* We instantiated two gradient objects $x_{1}$ and $x_{2}$ (here just lists).  These objects have two name attributes.  The first attribute is the function value and the second attribute is the derivative.  When performing a multiplication, the usual multiplication operator can be used for the function value part of the object.  However, for the derivative part, one must overload the multiplication operator.  Note that this could be accomplished in a class by implementing the `__mul__` and `__rmul__` special methods and returning the gradient object (like we did for `__add__` in the `Complex` class example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Your turn.  Using the same approach as above, write some code to do forward mode automatic differentiation of the function $$f\\left(x\\right) = x^{r}$$ where $r \\in \\mathbb{R}$.  Evaluate the derivative and function at $a = 3$ and $r = 4$.\n",
    "\n",
    "You can turn this into a function or closure if that helps you with the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on the Reverse Mode\n",
    "The focus of this class is on the forward mode of automatic differentiation.  The reverse mode is also extremely popular and useful (e.g. in scenarios that have $f:\\mathbb{R}^{m}\\mapsto\\mathbb{R}$).  Here we will outline the mechanics of the reverse mode, show a little example, and survey the basic equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Sketch of the Reverse Mode\n",
    "* Create evaluation graph\n",
    "* Forward pass does function evaluations\n",
    "* Forward pass also does partial derivatives of elementary functions\n",
    "  * **It does NOT do the chain rule!**\n",
    "  * Just stores the partial derivatives\n",
    "  * If $x_{3} = x_{1}x_{2}$ is a node, we store $\\dfrac{\\partial x_{3}}{\\partial x_{1}}$ and $\\dfrac{\\partial x_{3}}{\\partial x_{2}}$.  That's it.\n",
    "* Reverse pass starts with $\\overline{x}_{N} = \\dfrac{\\partial f}{\\partial x_{N}} = 1$ (since $f$ is $x_{N}$)\n",
    "* Then it gets $\\overline{x}_{N-1} = \\dfrac{\\partial f}{\\partial x_{N}}\\dfrac{\\partial x_{N}}{\\partial x_{N-1}}$\n",
    "  * **Note:** $\\dfrac{\\partial x_{N}}{\\partial x_{N-1}}$ is already stored from the forward pass\n",
    "* The only trick occurrs when we get to a branch in the graph.  That is, when the node we're on has more than one child.  In that case, we sum the two paths.  For example, if $x_{3}$ has $x_{4}$ and $x_{5}$ as children, then we do $$\\overline{x}_{3} = \\dfrac{\\partial f}{\\partial x_{3}} = \\dfrac{\\partial f}{\\partial x_{4}}\\dfrac{\\partial x_{4}}{\\partial x_{3}} + \\dfrac{\\partial f}{\\partial x_{5}}\\dfrac{\\partial x_{5}}{\\partial x_{3}}.$$\n",
    "  * **Note:** This summation is a manifestation of the chain rule.\n",
    "\n",
    "\n",
    "The reverse mode cannot be interpreted in the context of dual numbers like the forward mode was.  The little implementation sketch that we did for the forward mode will need to be generalized for the reverse mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic Equations\n",
    "These equations are modified from Nocedal and Wright (page 180).\n",
    "\n",
    "The partial derivative of $f$ with respect to $x_{i}$ can be written as \n",
    "\\begin{align}\n",
    "  \\dfrac{\\partial f}{\\partial x_{i}} = \\sum_{\\text{j a child of i}}{\\dfrac{\\partial f}{\\partial x_{j}}\\dfrac{\\partial x_{j}}{\\partial x_{i}}}.\n",
    "\\end{align}\n",
    "At each node $i$ we compute \n",
    "\\begin{align}\n",
    "  \\overline{x}_{i} += \\dfrac{\\partial f}{\\partial x_{j}}\\dfrac{\\partial x_{j}}{\\partial x_{i}}.\n",
    "\\end{align}\n",
    "The $\\overline{x}_{i}$ variable stores the current value of the partial derivative at node $i$.  It is sometimes called the adjoint variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example for Intuition\n",
    "Let's try to evaluate the function $$f\\left(x,y\\right) = xy + \\exp\\left(xy\\right)$$ and its gradient at the point $a = (1,2)$.  We'll use the reverse mode this time.\n",
    "\n",
    "Clearly we have $$\\nabla f = \\begin{bmatrix} y + \\exp\\left(xy\\right)y \\\\ x + \\exp\\left(xy\\right)x \\end{bmatrix}.$$  Hence\n",
    "\\begin{align}\n",
    "  f\\left(a\\right) &= 2 + e^{2} \\\\\n",
    "  \\nabla f &= \n",
    "  \\begin{bmatrix} \n",
    "    2 + 2e^{2} \\\\ \n",
    "    1 + e^{2} \n",
    "  \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Here's a visualization of the computational graph:\n",
    "\n",
    "![](figs/Reverse-Example.png)\n",
    "\n",
    "Let's use the reverse mode to calculate the gradient of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate the forward trace and calculate the partial derivatives of a node wrt its children\n",
    "  * Notice that this time we need to save the graph\n",
    "2. Start at $x_{5}$ and start calculating the chain rule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
